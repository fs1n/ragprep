# HTML to RAG Preprocessor

A lightweight, flexible preprocessing pipeline to convert raw HTML content (from forums, blogs, documentation, etc.) into structured JSONL format for use in Retrieval-Augmented Generation (RAG) pipelines. This tool cleans HTML, extracts useful metadata, splits the content into semantically coherent chunks, and writes it in a machine-readable format.

---

## âœ¨ Features

* ğŸ§¹ **HTML Cleanup**: Removes scripts, styles, boilerplate (headers/footers/navbars), and leaves clean readable text.
* ğŸ·ï¸ **Metadata Extraction**: Automatically extracts document title, author, date if available.
* ğŸ“š **Chunking**: Uses LangChain's `RecursiveCharacterTextSplitter` to intelligently break content into overlapping chunks for RAG compatibility.
* ğŸ“¤ **JSONL Output**: Outputs each text chunk and its metadata as a JSON object in a newline-delimited file.
* âœ… **Supports batch input**: Accepts individual files or directories with nested HTML content.

---

## ğŸ“¦ Requirements

```bash
pip install -r requirements.txt
```

**Dependencies:**

* `beautifulsoup4`
* `langchain_text_splitters`
* `tqdm`

---

## ğŸš€ Usage

```bash
python preprocess_html.py \
    --input path/to/html_or_folder [additional_paths] \
    --output output.jsonl \
    --chunk_size 1000 \
    --chunk_overlap 200
```

### ğŸ”§ Arguments

| Argument          | Description                                                     |
| ----------------- | --------------------------------------------------------------- |
| `--input` / `-i`  | One or more HTML files or directories to process.               |
| `--output` / `-o` | Path to the output `.jsonl` file. Default: `output.jsonl`       |
| `--chunk_size`    | Max number of characters per chunk. Default: `1000`             |
| `--chunk_overlap` | Number of overlapping characters between chunks. Default: `200` |

---

## ğŸ“‚ Input Structure

You can pass:

* One or more `.html` or `.htm` files.
* One or more directories (recursively scans for HTML files).

Example directory:

```
data/
â”œâ”€â”€ blog1.html
â”œâ”€â”€ forum/
â”‚   â”œâ”€â”€ thread1.html
â”‚   â””â”€â”€ thread2.html
â””â”€â”€ docs/
    â””â”€â”€ guide.html
```

Command:

```bash
python preprocess_html.py -i data/ -o chunks.jsonl
```

---

## ğŸ“¤ Output Format

Each line in the output `.jsonl` file is a JSON object:

```json
{
  "content": "Chunk of cleaned text content...",
  "metadata": {
    "filename": "guide.html",
    "title": "Getting Started Guide",
    "author": "John Doe",
    "date": "2024-05-01T12:00:00Z",
    "chunk": 0
  }
}
```

---

## ğŸ§  Example Use Case

This script is ideal for preparing a knowledge base from HTML content to feed into:

* ğŸ§  **RAG pipelines**
* ğŸ” **Vector databases** (Chroma, FAISS, Pinecone)
* ğŸ’¬ **Chatbots with context-aware memory**
* ğŸ“š **Search indexers**

---

## ğŸ› ï¸ Advanced Tips

* To scrape a website first, use tools like:

  * [`wget --mirror`](https://www.gnu.org/software/wget/manual/html_node/Mirroring.html)
  * [`httrack`](https://www.httrack.com/)
  * Custom scrapers with `requests` + `BeautifulSoup`

* After generating `output.jsonl`, feed it into your embedding + vector store setup.

---

## ğŸ“„ License

MIT License
